\chapter{Expectation-maximization algorithm}

The generic framework for expectation-maximization (EM) algorithms introduced
by \cite{bucstuvog15} applies to both language models and translation models.
This chapter will introduce terminology and notation from \cite{bucstuvog15} as
far as is necessary to apply this framework to the Hidden Markov Model in the
subsequent chapters.

\section{Preliminaries}

The set $\brc{0,1,2,\ldots}$ of non-negative integers and the set of
non-negative reals shall be denoted by $\zn$ and $\zr_{\geq0}$, respectively.
We assume that
\begin{align*}
 0^0 &:= 1, &
 \log 0 &:= -\infty, &
 0 (-\infty) &= \log 0^0 = 1.
\end{align*}

\begin{definition}
 Given a countable set $X$, a mapping $c: X \to \zr_{\geq0}$ is called a \emph{$X$-corpus}.
\end{definition}

When used as input for a language model's training algorithm, $X$ is the set of
all sentences consisting of words from the language in question, and $c(x)$
describes how often a sentence $x\in X$ occurs in the corpus $c$. The
\emph{size} of the corpus is defined as
\begin{equation*}
 \abs c := \sum_{x\in X} c(x).
\end{equation*}

\begin{definition}
 A \emph{probability distribution of $X$} is an $X$-corpus of size 1.
\end{definition}

The set of all such probability distributions is denoted by $\um(X)$.
Probability distributions can be derived from corpora:

\begin{definition}
 Given a non-empty and finite $X$-corpus $c$, the \emph{empirical probability
 distribution} $\tilde c$ is defined as
 \[
  \tilde c(x) = \frac{c(x)}{\abs c}.
 \]
\end{definition}

This expression is not well-defined for $\abs c = 0$ or $\abs c = \infty$,
hence the requirement for $c$ to be non-empty and finite. If this cannot be
guaranteed, a fall-back function can be added.

\begin{definition}
 Given $p\in\um(X)$, a \emph{normalization mapping with fall-back $p$} is the mapping
 \[
  \overline p: \zr_{\geq0}^X \to \um(X),
  \quad
  c \mapsto \begin{cases}
   \tilde c & \text{if } 0 < \abs c < \infty, \\ p & \text{otherwise}.
  \end{cases}
 \]
\end{definition}

\begin{definition}
 Given an $X$-corpus $c$ and $p\in\um(X)$, the \emph{likelihood of $c$ under $p$} is
 \[
  p(c) := \prod_{x\in X} p(x)^{c(x)}.
 \]
\end{definition}

The likelihood describes the probability of observing the sentences from the
corpus $c$ when sentences occur with the probability distribution described by
$p$. A training algorithm will take $c$ as an input, and seek to find an
admissible $p$ such that $p(c)$ is maximized. If any $p$ is admissible, then
for non-empty and finite $c$, the optimal choice is $p = \tilde c$:

\begin{lemma}\label{lemma:empirical1}
 Let $c$ be a non-empty and finite $X$-corpus. Then $\tilde c(c) \geq p(c)$ for every $p\in\um(X)$.
\end{lemma}

\begin{proof}
 Since $\log$ is monotone, it suffices to show that $\log\tilde c(c) \geq \log p(c)$. Using Gibbs' inequality,
 \begin{align*}
  \log \tilde c(c)
  &= \sum_{x\in X} c(x) \cdot \log \tilde c(x)
  = \abs c \cdot \sum_{x\in X} \tilde c(x) \cdot \log \tilde c(x) \\
  &\geq \abs c \cdot \sum_{x\in X} \tilde c(x) \cdot \log p(x)
  = \sum_{x\in X} c(x) \cdot \log p(x)
  = p(c)
  \qedhere
 \end{align*}
\end{proof}

However, using $p = \tilde c$ directly is not useful because this probability
distribution is grossly overfitted: It will assign zero probability to any
sentence not in the original corpus. A useful language model thus limits the
set of admissible $p$ by describing the probability distribution in terms of
\emph{model parameters} $\omega\in\Omega$.

\begin{definition}
 Given a set $\Omega$, an \emph{$\Omega$-probability model for $X$} is a mapping $p:\Omega\to\um(X)$.
\end{definition}

Instead of $p(\omega)$, we write $p_\omega$. Training shall then find $\omega\in\Omega$ such that $p_\omega(c)$ is maximized.

\begin{definition}
 Given a set $\Omega$ and an $\Omega$-probability model $p$ for $X$, the
 \emph{maximum likelihood estimator} for $p$ is the mapping
 \[
  \mle_p: \zr_{\geq0}^X \to \up(\Omega),
  \quad
  c \mapsto \argmax_\omega p_\omega(c).
 \]
\end{definition}

$\mle_p(c)$ is the set of all $\omega$ with maximal likelihood, but training
only needs to find a single $\hat\omega \in \mle_p(c)$. Computing $\mle_p(c)$
by brute force is typically not tractible because the set $\Omega$ is infinite
(usually countably infinite). However, there is one easily solvable special
case.

\begin{lemma}\label{lemma:empirical2}
 Let $c$ be a finite $X$-corpus, and $p$ a $\Omega$-probability model for $X$.
 If there exists $\hat\omega\in\Omega$ such that $p_{\hat\omega} = \tilde c$,
 then $\hat\omega\in\mle_p(c)$.
\end{lemma}

\begin{proof}
 If $c$ is empty, then $p_\omega(c) = 1$ for every $\omega$, and thus $\mle_p(c) = \Omega \ni \hat\omega$. Otherwise, by Lemma~\ref{lemma:empirical1}, $p_{\hat\omega}(c) = \tilde c(c) \geq p_\omega(c)$ for every $\omega\in\Omega$, and thus $\hat\omega \in \mle_p(c)$.
\end{proof}

\section{Algorithmic skeleton}

\begin{algorithm}[t]
 \caption{Algorithmic skeleton for EM of language models according to \cite{bucstuvog15}}
 \label{alg:skeleton}
 \begin{algorithmic}[1]
  \algorithmheader[Input:] $X$-corpus $c$
  \algorithmheader         $\Omega$-probability model $p$ for $Y\times X$
  \algorithmheader         some initial parameter $\omega_0 \in \Omega_0$ where $\Omega_0 := \brc{\omega\in\Omega: p_\omega(c) \neq 0}$
  \algorithmheader[Implicit:] step mapping $\psi:\Omega_0\to\up(\Omega)$
  \algorithmheader            \hspace{1em} such that $\omega'\in\psi(\omega)$ implies $p_\omega(c) \leq p_{\omega'}(c)$
  \algorithmheader[Output:] sequence $\omega_1,\omega_2,\ldots\in\Omega_0$
  \algorithmheader            \hspace{1em} such that $p_{\omega_0}(c), p_{\omega_1}(c), p_{\omega_2}(c)$ nondecreasing

  \STATE $i\leftarrow 0$
  \WHILE{not converged}
   \STATE $\omega_{i+1} \leftarrow \text{select a member of $\psi(\omega_i)$}$
   \STATE output $\omega_{i+1}$
   \STATE $i\leftarrow i+1$
  \ENDWHILE
 \end{algorithmic}
\end{algorithm}

The major complication that occurs when trying to train a typical language
model is that not all required data is present in the corpus. For example, a
probabilistic context-free grammar is described by the probability distribution
of derivation rules. \cite{laryou90} When the training data consists of full
parse trees (\emph{supervized training}), the optimal probability distribution
can be found by simply counting how many times each rule is used across all
these parse trees, and then computing the empirical probability distribution
for this corpus. Most of the times, however, the training data will consist
only of sentences. The information about how to parse the sentences is hidden.

The same problem arises with the Hidden Markov Model: When training data is not
already annotated with state information, the information which states
correspond to which words from the training data remains hidden. Expectation
maximization algorithms can be used when parts of the training data are hidden
in such a way.

For the remainder, let
\begin{itemize}\setlength\itemsep{-0.3em}
 \item $X$ and $Y$ be countable sets,
 \item $\Omega$ be a set,
 \item $c$ be a finite $X$-corpus and
 \item $p$ be a $\Omega$-probability model for $Y\times X$.
\end{itemize}

$c$ represents the set of training data. Each $x\in\operatorname{supp}(c)$ is
an \emph{observation}. To judge its probability under a $p_\omega$, additional
\emph{hidden information} $y\in Y$ is required. For notational convenience, we define
\begin{align*}
 p_\omega(c) &:= \prod_{x\in X} p_\omega(x)^{c(x)}, &
 \text{where } p_\omega(x) &:= \sum_{y\in Y} p_\omega(x,y).
\end{align*}

That is, even though $p_\omega$ is a probability distribution over $Y\times X$,
we allow to take the likelihood of the $X$-corpus $c$ under $p$ by aggregating
the probabilities for all hidden information $y$ that lead to a certain
observation $x$.

The basic pattern for expectation maximization is outlined in
algorithm~\ref{alg:skeleton}. The algorithm starts with an initial $\omega_0$
such that $p_{\omega_0}(c) \neq 0$. It then iteratively employs a step mapping
to choose the next $\omega_i$ with a higher (or at least equal) likelihood than
the one that came before.

\begin{definition}
 A \emph{step mapping} is a mapping $\psi:\Omega_0\to\up(\Omega)$ which is nondecreasing in the following manner:
 \[
  \forall \omega\in\Omega_0: \forall \omega'\in\psi(\Omega): p_\omega(c) \leq p_{\omega'}(c)
 \]
\end{definition}

The step mappings that we will consider will typically consist of two steps:
\begin{enumerate}
 \item \emph{Expectation:} The training data $c$ is converted into a
  \emph{complete-data corpus}. Using the $\omega_i$ from the previous
  iteration, the complete-data corpus estimates how hidden information
  contributes to the observations in the original corpus.
 \item \emph{Maximization:} A suitable maximum-likelihood estimator is applied
  to the complete-data corpus to choose $\omega_{i+1}$.
\end{enumerate}

%TODO: this section may use some citations because it makes claims
This back and forth of using the current $\omega$ to enrich the training data
and using the enriched data to find a better $\omega$ will converge towards a
local maximum of likelihood. The iteration is therefore usually aborted after
the desired running time has been exceeded, or after the changes of
$p_{\omega_i}(c)$ per iteration have become smaller than some threshold.

\cite{bucstuvog15} identify three types of step mappings that build on each
other, each one more specific than the one before it. Since the training of
Hidden Markov Models will be identified as an instance of the most specific
step mapping, the remainder of this chapter will introduce all three in order.

\section{Corpus-based step mapping}

The most general type of complete-data corpus can be obtained by distributing
$c(x)$ among the hidden information $y$ according to the probability
distribution $p_\omega$:
\[
 c\!\dangle{\omega,p}(y,x) := \begin{cases}
  c(x) \cdot \frac{p_\omega(y,x)}{p_\omega(x)} & \text{if } p_\omega(x) \neq 0, \\
  0 & \text{if } p_\omega(x) = 0.
 \end{cases}
\]
Recall that $p_\omega(x) = \sum_y p_\omega(y,x)$. Therefore,
$\abs{c\!\dangle{\omega,p}} = \abs c$. This corpus now has the correct
structure for plugging it into $\mle_p$, yielding the \emph{corpus-based step
mapping}\footnote{The proof that this step mapping is nondecreasing can be
found in \cite[pp.~10]{bucstuvog15}.}
\[
 \stepmap p_\mathrm{cb}: \Omega_0\to\up(\Omega),
 \quad
 \omega \mapsto \mle_p\mbig\kla{c\!\dangle{\omega,p}} = \argmax_{\omega'} p_{\omega'}\mbig\kla{c\!\dangle{\omega,p}}.
\]

For very simple language models, the $\argmax$ can be solved at this point
already. To apply lemma~\ref{lemma:empirical2}, $\hat\omega$ needs to be found
such that $p_{\hat\omega} = \widetilde{c\!\dangle{\omega,p}}$. This operation
is typically hard, which is why a more specific step mapping is helpful.

\section{Simple counting step mapping}

The next such step mapping requires the language model to be described by a
\emph{counting information}. Before defining this term, some additional
notation needs to be introduced.

\begin{definition}
 Let $A$ and $B$ be sets. A mapping $p: B \to \um(A)$ is called
 \emph{conditional probability distributions of $A$ given $B$}. The set of all
 such mappings is denoted by $\um(A|B)$.
\end{definition}

To simply notation, we define $p(a|b) := p(b)(a)$.

\begin{definition}
 Let $C\subseteq A\times B$ be a set. Then
 \[
  \um_C(A|B) := \brc{p\in\um(A|B): \operatorname{supp}(p)\subseteq C}
 \]
 is the set of all conditional probability distributions of $A$ given $B$
 constrained to $C$.
\end{definition}

\begin{definition}
 Given a set $\Omega$, a \emph{conditional $\Omega$-probability model for $A$
 and $B$ (constrained to $C$)} is a mapping $q:\Omega\to\um(A|B)$ (or
 $q:\Omega\to\um_C(A|B)$).
\end{definition}

When talking about counting informations (and inside-outside informations in
the next section), we assume the previously established requirements for $X$,
$Y$, $\Omega$, $c$ and $p$. Furthermore, we require that $X$ and $Y$ both
contain a special symbol $\bot$ such that $c(\bot) = 0$. $\bot$ can easily be
added to any previously defined $X$ and $Y$ without affecting the requirement
for countability. The notation $U_{\not\bot} := U\setminus\brc\bot$ shall be
defined for any set $U$.

\begin{definition}
 Let $A$, $B$ and $C$ be sets such that $C\subseteq A\times B$. A
 \emph{counting information} is a triple $\varkappa = (q,\lambda,\pi)$ such
 that
 \begin{align*}
  q &: \Omega\to\um_C(A|B), &
  \lambda &: X_{\not\bot} \times Y_{\not\bot} \to [0,1], &
  \pi &: X_{\not\bot} \times Y_{\not\bot} \to \zr_{\geq0}^C.
 \end{align*}
\end{definition}

The motivation for this definition is to model hidden information $y$ as
consisting of countable events $c\in C$. $\lambda(x,y)$ describes whether (and,
possibly, with what probability) a certain $y$ can be the cause for a certain
observation $x$.\footnote{Most actual instances of $\lambda$ use only integer
images, i.~e.~$\lambda(X_{\not\bot}\times Y_{\not\bot}) = \brc{0,1}$, thus
following this intuitive notion. However, the possibility of using fractional
values for $\lambda(x,y)$ is occasionally useful, e.~g.~to define a counting
information for the IBM Model 1 in \cite[pp.~23]{bucstuvog15}.} For
$\lambda(x,y)>0$, $\pi(x,y)$ is a $C$-corpus that describes how often each
countable event occurs in this hidden information.

Following the assumption that the countable events $C$ fully encode the hidden
information $Y$, we can use these intuitive notions to describe the original
probability model $p$ in terms of the counting information.

\begin{definition}
 Given $\varkappa=(q,\lambda,\pi)$, the \emph{induced model} $\varkappa^\flat:\Omega\to\zr_{\geq0}^{Y\times X}$ is given by
 \[
  (\varkappa^\flat)_\omega(y,x) := \begin{cases}
   \lambda(x,y) \cdot q_\omega\mbig\kla{\pi(x,y)} & \text{if } x,y\neq\bot, \\
   1 - \sum_{x',y'\neq\bot} \lambda(x',y') \cdot q_\omega\mbig\kla{\pi(x',y')} & \text{if } x = y = \bot, \\
   0 & \text{otherwise}.
  \end{cases}
 \]
\end{definition}

This definition shows why the introduction of $\bot$ into $X$ and $Y$ was
useful. By defining $(\varkappa^\flat)_\omega(\bot,\bot)$ as above, we ensure
$\mnorm\abs{(\varkappa^\flat)_\omega} = 1$. Therefore, $\varkappa^\flat$ is an
$\Omega$-probability model for $Y\times X$ iff
$(\varkappa^\flat)_\omega(\bot,\bot) \geq 0$. We call $\varkappa$ \emph{proper}
in this case.

Since we now have a probability model $p = \varkappa^\flat$ as required by the
corpus-based step mapping, we can lift its complete-data corpus into the domain
of the counting information, obtaining a new complete-data corpus
\[
 c\!\dangle{\omega,\varkappa}: C \to \zr_{\geq0}, \quad
 (a,b) \mapsto \sum_{x,y} c\mnorm\dangle{\omega,\varkappa^\flat}(y,x) \cdot \pi(x,y)(a,b).
\]

We can now apply a maximum-likelihood estimator for $q$ to arrive at the step
mapping for this class of language models.

\begin{definition}
 Given a set $\Omega$ and a conditional $\Omega$-probability model $q$ for $A$
 and $B$, the \emph{conditional maximum likelihood estimator} for $q$ is the mapping
 \[
  \cmle_q: \zr_{\geq0}^{A\times B} \to \up(\Omega),
  \quad
  c \mapsto \argmax_\omega q_\omega(c).
 \]
\end{definition}

Using this definition, the \emph{simple counting step mapping}\footnote{The
proof that this step mapping is equivalent to
$\mnorm\stepmap{\varkappa^\flat}_\mathrm{cb}$, and thus also nondecreasing, can
be found in \cite[p.~13]{bucstuvog15}.} is
\[
 \stepmap\varkappa_\mathrm{sc}: \Omega_0 \to \up(\Omega), \quad
 \omega \to \cmle_q\mbig\kla{c\mnorm\dangle{\omega,\varkappa}} = \argmax_{\omega'} q_{\omega'}\mbig\kla{c\mnorm\dangle{\omega,\varkappa}}.
\]
The simple counting step mapping has two advantages over
$\stepmap\cdot_\mathrm{cb}$: First, many language models can be described in
terms of the countable events only, such that $\Omega = C$. In this case,
lemma~\ref{lemma:empirical2} can be used to solve the $\argmax$ by simply
computing the empirical probability distribution of
$c\!\dangle{\omega,\varkappa}$.

Second, even if this is not possible, the set of countable events $C$ is
usually much smaller than the set of all observations $X$ or hidden information
$Y$, making the evaluation of the $\argmax$ more tractable than for the
corpus-based step mapping. For example, when considering probabilistic
context-free grammars, $X$ (the set of all sentences) and $Y$ (the set of all
parse trees) are both countably infinite, but $C$ (the set of all derivation
rules) is finite.
