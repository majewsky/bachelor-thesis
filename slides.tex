\documentclass{beamer}

% encoding
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% language
\usepackage[ngerman]{babel}
% beamer theme
\usetheme{default}
\beamertemplatenavigationsymbolsempty
\hypersetup{pdfpagemode=UseNone} % don't show bookmarks on initial view
% fonts
\usepackage{mathpazo}
\usefonttheme{serif}
% for being able to copy text out of the PDF
\usepackage{cmap}

% math mode formatting etc.
\usepackage{amsmath,amsfonts,amssymb,amsthm,sistyle,xcolor,delim}
\SIstyle{German}
\usepackage{graphicx,tikz,calc,tabularx,hhline}
\usetikzlibrary{backgrounds,calc,decorations.pathreplacing,positioning}

\tikzset{
 >=stealth,
}

% metadata
\title{Training of Hidden Markov models as an instance of the expectation maximization algorithm}
\author{Stefan Majewsky}
\date{22. August 2017}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}\frametitle{Gliederung}
 Sprachmodelle
 \begin{itemize}
  \item Bigramm-Modell
  \item Hidden-Markov-Modell
 \end{itemize}
 \pause
 EM-Algorithmen
 \begin{itemize}
  \item Baum-Welch-Algorithmus
  \pause
  \item Inside-Outside-EM-Algorithmus
  \item Instanziierung f체r Hidden-Markov-Modelle
 \end{itemize}
\end{frame}

\begin{frame}\frametitle{Sprachmodell}
 \begin{itemize}
  \item Ziel: probabilistische Beschreibung einer Sprache
   \[
    \text{Satz } x \mapsto \text{Wahrscheinlichkeit } p(x)
    \vspace{-1em}
   \]
  \pause\item beschrieben durch \emph{Modellparameter} $\omega\in\Omega$
   \[\begin{tikzpicture}[c/.style={rectangle,draw},l/.style={anchor=east},r/.style={anchor=west}]
    \node[l] (corpus)   at (0,+0) { \strut Korpus };
    \node[c] (training) at (2,+0) { \strut Training };
    \node[r] (omega)    at (4,+0) { \strut Parameter $\omega$ };
    \draw[->] (corpus) -- (training);
    \draw[->] (training) -- (omega);
    \node[l] (input)    at (0,-2) { \strut Satz $x$ };
    \node[c] (eval)     at (2,-2) { \strut Evaluierung };
    \node[r] (output)   at (4,-2) { \strut $p(x)$ };
    \draw[->] (input) -- (eval);
    \draw[->] (eval) -- (output);
    \draw[->] (omega.south) |- ($(eval.north)!0.5!(omega.south)$) -| (eval.north);
   \end{tikzpicture}\]
 \end{itemize}
\end{frame}

\begin{frame}\frametitle{Bigramm-Modell}
 \begin{itemize}
  \item Idee: Wahrscheinlichkeit jedes Wortes h채ngt nur vom vorhergehenden Wort ab
   \pause
   \[\begin{tikzpicture}[node distance=10mm]
    \node at (0,0) {~};
    \node (x0) at (1.2,0) { \strut \# };
    \node (x1) at (3.25,0) { \strut Alice };
    \node (x2) at (5.7,0) { \strut sees };
    \node (x3) at (7.95,0) { \strut Bob };
    \node (x4) at (9.6,0) { \strut \# };
    \node at (10,0) {~};
    \draw[decorate,decoration={brace,amplitude=5pt,mirror}] ($(x0.south)!0.3!(x0.south east)+(0,5pt)$) -- ($(x1.south)!0.3!(x1.south west)+(0,5pt)$);
    \draw[decorate,decoration={brace,amplitude=5pt,mirror}] ($(x1.south)!0.3!(x1.south east)+(0,5pt)$) -- ($(x2.south)!0.3!(x2.south west)+(0,5pt)$);
    \draw[decorate,decoration={brace,amplitude=5pt,mirror}] ($(x2.south)!0.3!(x2.south east)+(0,5pt)$) -- ($(x3.south)!0.3!(x3.south west)+(0,5pt)$);
    \draw[decorate,decoration={brace,amplitude=5pt,mirror}] ($(x3.south)!0.3!(x3.south east)+(0,5pt)$) -- ($(x4.south)!0.3!(x4.south west)+(0,5pt)$);
   \end{tikzpicture}\vspace*{-2em}\]
   \pause
   \[
    p(x) = b(\text{Alice}|\#) \cdot b(\text{sees}|\text{Alice}) \cdot b(\text{Bob}|\text{sees}) \cdot b(\#|\text{Bob})
   \]
 \end{itemize}
\end{frame}

\begin{frame}\frametitle{Bigramm-Modell: Training}
 \begin{itemize}
  \item Korpus: "`Alice sees Bob"', "`Alice likes cake"'
 \end{itemize}
 \begin{center}\begin{tabular}{c|c|c}
  $x$   & \only<1>{\color{white}}$\operatorname{count}(\text{Alice}\;x)$ & \only<-2>{\color{white}}$b(x|\text{Alice})$ \\\hline
  Alice & \only<2->{0} & \only<3->{0}         \\
  sees  & \only<2->{1} & \only<3->{\num{0.5}} \\
  Bob   & \only<2->{0} & \only<3->{0}         \\
  likes & \only<2->{1} & \only<3->{\num{0.5}} \\
  cake  & \only<2->{0} & \only<3->{0}         \\
 \end{tabular}\end{center}
\end{frame}

\end{document}\endinput

Gliederung/Zeitbudget:

- (3 Min) Idee Sprachmodell: P(x), vllt Bigramm-Modell zur Veranschaulichung
- (5 Min) Hidden Markov Model: H = (Q,V,#,t,e) -> P(x)
- (5 Min) Training: Idee Baum-Welch-Algorithmus bzw. EM allgemein -> gute Konvergenz, verallgemeinerbar?
- (10 Min) EM-Algorithmus aus [BSV15]; nur IO-Info, wrsl. eher ph채nomenologisch (am Beispiel PCFG) als exakt
- (7 Min) Anwendung auf HMM; B채ume aus K bzw. H(x)
